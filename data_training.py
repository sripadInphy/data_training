# -*- coding: utf-8 -*-
"""data_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13zBt7GYsxdZIeziruRRs-woDMrcUpHqz
"""

### Import Requirements ###
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D
from keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from joblib import dump,load
import csv
import os
import pickle as pkl
import random




'''
Model Creation
'''

def create_cnn():

    with tf.device('/GPU:0'):
        CNN_Classifier = Sequential()
        #### FIRST LAYER ####
        CNN_Classifier.add(Conv1D(64,(5), input_shape=(3000,1)))
        CNN_Classifier.add(Activation("relu"))
        CNN_Classifier.add(MaxPooling1D(pool_size = (2)))
        CNN_Classifier.add(Flatten())

        #### SECOND LAYER ####
        CNN_Classifier.add(Dense(64))
        CNN_Classifier.add(Activation("relu"))
        CNN_Classifier.add(Dropout(0.5))
        
        #### THIRD LAYER ####
        CNN_Classifier.add(Dense(32))
        CNN_Classifier.add(Activation("relu"))
        CNN_Classifier.add(Dropout(0.5))
        


        #### OUTPUT LAYER ####
        CNN_Classifier.add(Dense(18))
        CNN_Classifier.add(Activation('sigmoid'))

    es =  EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)
    fname = os.path.sep.join(['C:/theCave/ISO-ID/data_prep/weights/',
        "weights-{epoch:03d}-{val_loss:.4f}.hdf5"])


    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        fname,
        save_weights_only=True,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True,
        verbose=1)


    with tf.device('/GPU:0'):
        CNN_Classifier.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 1e-5, decay = 1e-6),loss = 'categorical_crossentropy', metrics = ['categorical_accuracy', 'accuracy', 'mse'])
    
    return CNN_Classifier,es,model_checkpoint_callback



'''
Data Processing.

- Combines individual isotope csv files to create training csv file and add
a column to label the spectrum.

- From consolidated csv file split the data set to traininf and validation
set.
'''
label_name = ['U238', 'Tc99m', 'Pu240', 'Cs137', 'Co60', 'K40', 'U233', 'Co57', 'Th232', 'Am241', 
                'Pu239', 'I131', 'Ra226', 'Ga67', 'Ir192', 'Ba133', 'U235', 'Cf252']
label_number = LabelEncoder().fit_transform(label_name)





#Function to create consolidated dataset
def consolidated_data(folder_path, output_file):
    # create the header for the output file
    header_written = False
    count = 1
    with open(output_file, 'w', newline='') as f_out:
        csv_writer = csv.writer(f_out)

        for filename in os.listdir(folder_path):
            if filename.endswith(".csv") and filename != output_file:
                with open(os.path.join(folder_path, filename), 'r') as f_in:
                    csv_reader = csv.reader(f_in)
                    for row in csv_reader:
                        label = filename.split()[0]
                        row.append(label)
                        csv_writer.writerow(row)
                        count = count + 1


#Function to create consolidated dataset for pickle files

def consolidated_data_pkl(folder_path, output_file):
    count = 0
    print("Labels : ", label_name)
    print("Label Index : ", label_number)
    with open(output_file, 'w', newline='') as f_out:
        csv_writer = csv.writer(f_out)

        for filename in os.listdir(folder_path):
            if filename.endswith(".pkl") and filename != output_file:
                with open(os.path.join(folder_path, filename), 'rb') as f_in:
                    data = pkl.load(f_in)
                    name_col = np.full((data.shape[0],1),label_number[label_name.index(filename.split('.')[0])])
                    data_with_name = np.concatenate((data,name_col),axis=1)
                    csv_writer.writerows(data_with_name)




#Function to splitting the dataset to training and validation set.
#input : consolidated csv file containing all spectrum with labels
def split_dataset(input_file):           
    line_offsets = list()
    with open(input_file,'r') as f:
        title = f.readline()
        # store offset of the first
        while True:
            # store offset of the next line start
            line_offsets.append(f.tell())
            line = f.readline()
            if line=="":
                break

        # now shuffle the offsets
        random.shuffle(line_offsets)
        print("len :", len(line_offsets))

        # and write the output file
        with open("train_select.csv", 'w') as fw:
            fw.write(title)
            for offset in line_offsets[0:int(0.8*len(line_offsets))]: #for offset in line_offsets[x*0.8]:
                
                f.seek(offset)
                fw.write(f.readline())

        with open("validation_select.csv", 'w') as fw:
            for offset in line_offsets[int(0.8*len(line_offsets)):-1]: 
                
                f.seek(offset)
                fw.write(f.readline())



'''
Data training : 
- Create mechanism to deal with large dataset. 
- Train and test model.
'''

#Function to read large csv file. To be used to model's generator function for training.
def read_csv(filename):
    with open(filename, 'r') as f:
        reader = csv.reader(f)
        count = 0

        glob_labels  = []
        glob_features = []
        for line in reader:
            # print('line : ' ,line)
            if count == 1 : 
                glob_features = []
                glob_labels = []
        # record = line.rstrip().split(',').astype(int)
            features = [int(float(n)) for n in line[:3000]]
            label = line[-1]
            features = np.array(features)
            features = features.reshape(3000,-1)
  
            yield features, tf.keras.utils.to_categorical(int(label), num_classes=18)


def generate_predictions(model, generator):
    X = []
    y = []
    for batch in generator:
        features, label = batch
        X.append(features)
        y.append(label)
    X = np.concatenate(X)
    y = np.concatenate(y)
    predictions = model.predict(X)
    return predictions, y



def main():


    folder_path = 'C:/theCave/ISO-ID/data_prep/output_data/single_isotope_data'
    output_file = 'C:/theCave/ISO-ID/data_prep/output_data/single_isotope_data/output.csv'
    #Merge data
    # consolidated_data_pkl(folder_path, output_file)
    print("....Finished merging dataset......")

    #Split data
    # split_dataset(output_file)
    print("....Finished splitting dataset......")

    #Read training dataset
    tf_ds = lambda: read_csv('C:/theCave/ISO-ID/train/train_select.csv')

    #Create Dataset using dataset generator 
    dataset = tf.data.Dataset.from_generator(tf_ds,output_signature=(tf.TensorSpec(shape=(3000,1), dtype=tf.uint16),tf.TensorSpec(shape=([18]), dtype=tf.uint8)))

    dataset = dataset.shuffle(1000)
    dataset = dataset.batch(256).prefetch(3)

    #Read Validation dataset
    val_ds = lambda: read_csv('C:/theCave/ISO-ID/train/validation_select.csv')
    val_ds = tf.data.Dataset.from_generator(val_ds, output_types = (tf.float32, tf.int64), output_shapes = (tf.TensorShape([3000,1]),tf.TensorShape([18])))

    val_ds = val_ds.batch(256).prefetch(3).repeat()
    cnn_model,es,model_checkpoint_callback = create_cnn()
    history = cnn_model.fit(dataset,validation_data = val_ds ,epochs = 100, callbacks = [es , model_checkpoint_callback], verbose=2)
    
    dump(cnn_model, 'C:/theCave/ISO-ID/train/trained_models/cnn.joblib')

    #Extract features using CNN Model
    cnn_predictions, cnn_labels = generate_predictions(cnn_model, dataset)

    # Flatten the features for SVM and Random Forest
    cnn_predictions = cnn_predictions.reshape(len(cnn_predictions), -1)
    # Make Labels a 1-D array 
    cnn_labels = np.argmax(cnn_labels, axis=1)
    #Train SVM model 
    svm = SVC(kernel='rbf', C=1.0, gamma='scale')
    svm.fit(cnn_predictions, cnn_labels)

    dump(svm, 'C:/theCave/ISO-ID/train/trained_models/cnn_svm.joblib')

    print("******* SVM trained **************")

    #Train Random  Forest model
    rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
    rf.fit(cnn_predictions, cnn_labels)
    dump(rf, 'C:/theCave/ISO-ID/train/trained_models/cnn_rf.joblib')

    print("******* Random Forest Done trained **************")
    print("Training finished")

if __name__ == "__main__":
    main()