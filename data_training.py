# -*- coding: utf-8 -*-
"""data_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13zBt7GYsxdZIeziruRRs-woDMrcUpHqz
"""

### Import Requirements ###
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D
from keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from joblib import dump,load
import keras.backend as K
import csv
import os
import pickle as pkl
import random
import datetime
import glob
import re
import ast
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression





no_of_bins = 1500
no_of_class = 9
pos_weight = 1
neg_weight = 1

'''
Model Creation
'''
def create_cnn():

    with tf.device('/GPU:0'):
        CNN_Classifier = Sequential()
        #### FIRST LAYER ####
        CNN_Classifier.add(Conv1D(32,(3), input_shape=(no_of_bins,1)))
        CNN_Classifier.add(Activation("relu"))
        CNN_Classifier.add(MaxPooling1D(pool_size = (2)))
        CNN_Classifier.add(Flatten())

        #### SECOND LAYER ####
        CNN_Classifier.add(Dense(64))
        CNN_Classifier.add(Activation("relu"))
        CNN_Classifier.add(Dropout(0.2))
        
        #### THIRD LAYER ####
        CNN_Classifier.add(Dense(32))
        CNN_Classifier.add(Activation("relu"))
        CNN_Classifier.add(Dropout(0.2))
        
        #### OUTPUT LAYER ####
        CNN_Classifier.add(Dense(no_of_class, activation='sigmoid'))
        
    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)
    fname = os.path.sep.join(['C:/theCave/ISO-ID/data_prep/weights/',
        "weights-{epoch:03d}-{val_loss:.4f}.hdf5"])


    log_dir = "C:/theCave/ISO-ID/train/logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        fname,
        save_weights_only=True,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True,
        verbose=1)

    with tf.device('/GPU:0'):
        # Define the weighted binary cross-entropy loss function
        def weighted_binary_crossentropy(y_true, y_pred):
            # Get the binary cross-entropy loss
            bce = K.binary_crossentropy(y_true, y_pred)

            # Apply the sample weights
            weight_vector = y_true * pos_weight + (1. - y_true) * neg_weight
            weighted_bce = weight_vector * bce

            # Return the mean over all samples
            return K.mean(weighted_bce)

        CNN_Classifier.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate = 1e-5, decay = 1e-6),
                               loss=weighted_binary_crossentropy, 
                               metrics=['categorical_accuracy', 'accuracy', 'mse'])
    
    return CNN_Classifier, es, model_checkpoint_callback, tensorboard_callback


'''
Model History 
evaluate the model
'''

def plot_model_history(model_name, history, epochs):
  
  print(model_name)
  plt.figure(figsize=(15, 5))
  
  # summarize history for accuracy
  plt.subplot(1, 2 ,1)
  plt.plot(np.arange(0, len(history['accuracy'])), history['accuracy'], 'r')
  plt.plot(np.arange(1, len(history['val_accuracy'])+1), history['val_accuracy'], 'g')
  plt.xticks(np.arange(0, epochs+1, epochs/10))
  plt.title('Training Accuracy vs. Validation Accuracy')
  plt.xlabel('Num of Epochs')
  plt.ylabel('Accuracy')
  plt.legend(['train', 'validation'], loc='best')
  
  plt.subplot(1, 2, 2)
  plt.plot(np.arange(1, len(history['loss'])+1), history['loss'], 'r')
  plt.plot(np.arange(1, len(history['val_loss'])+1), history['val_loss'], 'g')
  plt.xticks(np.arange(0, epochs+1, epochs/10))
  plt.title('Training Loss vs. Validation Loss')
  plt.xlabel('Num of Epochs')
  plt.ylabel('Loss')
  plt.legend(['train', 'validation'], loc='best')
  
  
  plt.show()



'''
Data Processing.

- Combines individual isotope csv files to create training csv file and add
a column to label the spectrum.

- From consolidated csv file split the data set to traininf and validation
set.
'''

# label_name = ['Cs137','Co60','K40','Co57','Am241','I131','Ir192','Ba133']
label_name = []
label_number = LabelEncoder().fit_transform(label_name)


def normalize(arr):
    """
    Normalizes an array by dividing each element by the sum of all elements.

    Args:
        arr (list): A list of numbers to normalize.

    Returns:
        list: A normalized list of numbers.
    """
    arr = [float(x) for x in arr]
    arr_sum = sum(arr)
    return [float(i) / arr_sum for i in arr]



elements = set()
# Regular expression to extract element names from filename
regex = r"\(\((.*?)\),"

#Function to create consolidated dataset
def consolidated_data(folder_path, output_file):
    folders = os.listdir(folder_path)
    global label_name
    for folder in folders:
        for name in folder.split(","):
            elements.add(name.replace("'","").replace("(","").replace(")","").replace(" ","")) 
    label_name = list(elements)    #Add all elements into a list
    for folder in folders:
        subfolders = os.listdir(os.path.join(folder_path+"/"+folder))
        with open(output_file, 'w', newline='') as f_out:
            csv_writer = csv.writer(f_out)
            for filename in subfolders:
                    split_name = re.findall(regex, filename) 
                    strength_str = re.search(r"array\((.*?)\)", filename).group(1)
                    strength_str = strength_str.replace('[', '').replace(']', '') # remove any unwanted characters
                    strength = [float(x.strip()) for x in strength_str.split(",")]
                    matches = split_name[0].replace("'","").replace(" ","").split(",")
                    indices = [label_name.index(e.strip("'")) for e in matches]
                    with open(os.path.join(folder_path+"/"+folder+"/"+filename), 'r') as f_in: 
                        print(filename)
                        csv_reader = csv.reader(f_in)
                        for row in csv_reader:
                            if len(row) > 0:
                                label = filename.split('.')[0]
                                row = normalize(row)
                                row = [float(val)*100000 for val in row]
                                row.append([strength[indices.index(i)] if i in indices else 0 for i in range(len(label_name))])
                                csv_writer.writerow(row)
                    






#Function to splitting the dataset to training and validation set.
#input : consolidated csv file containing all spectrum with labels
def split_dataset(input_file):           
    line_offsets = list()
    with open(input_file,'r') as f:
        title = f.readline()
        # store offset of the first
        while True:
            # store offset of the next line start
            line_offsets.append(f.tell())
            line = f.readline()
            if line=="":
                break

        # now shuffle the offsets
        random.shuffle(line_offsets)
        print("len :", len(line_offsets))

        # and write the output file
        with open("train_select.csv", 'w') as fw:
            fw.write(title)
            for offset in line_offsets[0:int(0.8*len(line_offsets))]: #for offset in line_offsets[x*0.8]:
                
                f.seek(offset)
                fw.write(f.readline())

        with open("validation_select.csv", 'w') as fw:
            for offset in line_offsets[int(0.8*len(line_offsets)):-1]: 
                
                f.seek(offset)
                fw.write(f.readline())



'''
Data training : 
- Create mechanism to deal with large dataset. 
- Train and test model.
'''


def read_csv(filename):
    with open(filename, 'r') as f:
        reader = csv.reader(f)
        count = 0

        for line in reader:
            features = [np.float64(n) for n in line[:no_of_bins]]
            label = ast.literal_eval(line[-1])  # safely evaluate string as Python expression
            label = np.array(label, dtype=np.float32)
            features = np.array(features)
            features = features.reshape(no_of_bins,-1)
            # features = features*1000  #scale it up cuz the model can't recognize small values
            # yield features, tf.keras.utils.to_categorical(int(label), num_classes=no_of_class)
            yield features, label


def generate_predictions(model, generator):
    X = []
    y = []
    for batch in generator:
        features, label = batch
        X.append(features)
        y.append(label)
    X = np.concatenate(X)
    y = np.concatenate(y)
    predictions = model.predict(X)
    return predictions, y



def main():


    if True:
        # folder_path = 'C:/theCave/ISO-ID/data_prep/output_data/single_isotope_data'
        # output_file = 'C:/theCave/ISO-ID/data_prep/output_data/single_isotope_data/output.csv'
        folder_path = 'C:/theCave/ISO-ID/data_prep/combos/combos_hamm'
        output_file = 'C:/theCave/ISO-ID/data_prep/output_data/combos_hamm/output.csv'

        
        #Merge data
        # consolidated_data_pkl(folder_path, output_file)
        # consolidated_data(folder_path, output_file)
        print(label_name)
        print("....Finished merging dataset......")

        #Split data
        # split_dataset(output_file)
        print("....Finished splitting dataset......")

        # #Read training dataset
        tf_ds = lambda: read_csv('C:/theCave/ISO-ID/train/train_select.csv')

        #Create Dataset using dataset generator 
        # dataset = tf.data.Dataset.from_generator(tf_ds,output_signature=(tf.TensorSpec(shape=(1500,1), dtype=tf.uint16),tf.TensorSpec(shape=([14]), dtype=tf.uint8)))
        dataset = tf.data.Dataset.from_generator(tf_ds,output_signature=(tf.TensorSpec(shape=(no_of_bins,1), dtype=tf.dtypes.float64), tf.TensorSpec(shape=(no_of_class,))))


        dataset = dataset.shuffle(1000)
        dataset = dataset.batch(256).prefetch(3)

        #Read Validation dataset
        val_ds = lambda: read_csv('C:/theCave/ISO-ID/train/validation_select.csv')
        # val_ds = tf.data.Dataset.from_generator(val_ds, output_types = (tf.float32, tf.int64), output_shapes = (tf.TensorShape([1500,1]),tf.TensorShape([14])))
        val_ds = tf.data.Dataset.from_generator(val_ds, output_signature=(tf.TensorSpec(shape=(no_of_bins,1),dtype=tf.dtypes.float64), tf.TensorSpec(shape=(no_of_class,))))


        val_ds = val_ds.batch(256).prefetch(3)
        cnn_model,es,model_checkpoint_callback,tboard = create_cnn()
        history = cnn_model.fit(dataset,validation_data = val_ds ,epochs = 100, callbacks = [es, model_checkpoint_callback,tboard], verbose=2)        #Early stopping removed
        
        dump(cnn_model, 'C:/theCave/ISO-ID/train/trained_models/cnn.joblib')

        plot_model_history('CNN', history.history,100)





        #Extract features using CNN Model
        cnn_predictions, cnn_labels = generate_predictions(cnn_model, dataset)

        # Flatten the features for SVM and Random Forest
        cnn_predictions = cnn_predictions.reshape(len(cnn_predictions), -1)
        # Make Labels a 1-D array 
        cnn_labels = np.argmax(cnn_labels, axis=1)
        #Train SVM model 
        svm = SVC(kernel='rbf', C=1.0, gamma='scale')
        svm.fit(cnn_predictions, cnn_labels)

        dump(svm, 'C:/theCave/ISO-ID/train/trained_models/cnn_svm.joblib')

    '''
    Other classifiers 
    '''
    if False:
        # initialize the classifiers
        knn = KNeighborsClassifier(n_neighbors=50)
        nb = GaussianNB()
        lr = LogisticRegression()
        svm_clf = SVC(kernel='linear')


        train_dataset = list(read_csv('C:/theCave/ISO-ID/train/train_select.csv'))
        train_features = np.array([x[0].reshape(no_of_bins,) for x in train_dataset])
        train_labels = np.array([x[1] for x in train_dataset])

        knn.fit(train_features, np.argmax(train_labels, axis=1))
        dump(knn, 'C:/theCave/ISO-ID/train/trained_models/knn.joblib')
        nb.fit(train_features, np.argmax(train_labels, axis=1))
        dump(nb, 'C:/theCave/ISO-ID/train/trained_models/nb.joblib')
        lr.fit(train_features, np.argmax(train_labels, axis=1))
        dump(lr, 'C:/theCave/ISO-ID/train/trained_models/lr.joblib')
        svm_clf.fit(train_features, np.argmax(train_labels, axis=1))
        dump(svm_clf, 'C:/theCave/ISO-ID/train/trained_models/svm.joblib')

        val_dataset = list(read_csv('C:/theCave/ISO-ID/train/validation_select.csv'))
        val_features = np.array([x[0].reshape(no_of_bins,) for x in val_dataset])
        val_labels = np.array([x[1] for x in val_dataset])

        knn_score = knn.score(val_features, np.argmax(val_labels, axis=1))
        nb_score = nb.score(val_features, np.argmax(val_labels, axis=1))
        lr_score = lr.score(val_features, np.argmax(val_labels, axis=1))
        svm_score = svm_clf.score(val_features, np.argmax(val_labels, axis=1))

        print("KNN score: ", knn_score)
        print("Naive Bayes score: ", nb_score)
        print("Logistic Regression score: ", lr_score)
        print("SVM score: ", svm_score)


    print("Training finished")

if __name__ == "__main__":
    main()